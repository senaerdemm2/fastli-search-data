{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c45a42-f18b-49d8-a101-15f9fb361e3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Visiting: https://fastli.pl/missions?page=1\n",
      "‚û°Ô∏è 6 listings found (page 1)\n",
      "üîé Visiting: https://fastli.pl/missions?page=2\n",
      "‚û°Ô∏è 6 listings found (page 2)\n",
      "‚ôªÔ∏è Same listings started repeating, scraping stopped.\n",
      "‚úÖ Scraping completed! 6 listings found. Saved to fastli_clean.csv.\n",
      "                     title           category              date   price  \\\n",
      "0  Best Mechanik in Warsaw       Rzeczoznawca  2025-09-10 11:29  219 z≈Ç   \n",
      "1            Ubezpieczenia             W≈Çasne  2025-09-10 11:29      z≈Ç   \n",
      "2              Nauka Boksu  Kursy i szkolenia  2025-09-10 11:29   30 z≈Ç   \n",
      "3        Serwis komputer√≥w        Inne/W≈Çasne  2025-09-10 11:29      z≈Ç   \n",
      "4         Mobilny mechanik             Serwis  2025-09-10 11:29  200 z≈Ç   \n",
      "\n",
      "  location  \n",
      "0     None  \n",
      "1  Rzesz√≥w  \n",
      "2    Guz√≥w  \n",
      "3  Gliwice  \n",
      "4  Gliwice  \n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "async def scrape_listings(base_url, max_pages=20):\n",
    "    data = []\n",
    "    seen_titles = set()  # to prevent duplicates\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        for page_num in range(1, max_pages + 1):\n",
    "            url = base_url.replace(\"page=1\", f\"page={page_num}\")\n",
    "            print(f\"üîé Visiting: {url}\")\n",
    "            await page.goto(url)\n",
    "\n",
    "            try:\n",
    "                await page.wait_for_selector(\"div.ps-missions__item, div.ps-mission\", timeout=15000)\n",
    "            except:\n",
    "                print(\"‚ùå No listings found, scraping stopped.\")\n",
    "                break\n",
    "\n",
    "            html = await page.content()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            listings = soup.find_all(\"div\", class_=\"ps-missions__item\")\n",
    "            print(f\"‚û°Ô∏è {len(listings)} listings found (page {page_num})\")\n",
    "\n",
    "            if not listings:\n",
    "                break\n",
    "\n",
    "            # If the first listing title was seen before -> repetition started, stop\n",
    "            first_title_el = listings[0].find(\"div\", class_=\"ps-mission__title\")\n",
    "            first_title = first_title_el.get_text(strip=True) if first_title_el else None\n",
    "            if first_title in seen_titles:\n",
    "                print(\"‚ôªÔ∏è Same listings started repeating, scraping stopped.\")\n",
    "                break\n",
    "\n",
    "            for item in listings:\n",
    "                # Title\n",
    "                title_el = item.find(\"div\", class_=\"ps-mission__title\")\n",
    "                title = title_el.get_text(strip=True) if title_el else None\n",
    "                seen_titles.add(title)\n",
    "\n",
    "                # Price\n",
    "                price_value = item.find(\"span\", class_=\"ps-price__value\")\n",
    "                price_currency = item.find(\"span\", class_=\"ps-price__curency\")\n",
    "                if price_value or price_currency:\n",
    "                    val = price_value.get_text(strip=True) if price_value else \"\"\n",
    "                    cur = price_currency.get_text(strip=True) if price_currency else \"\"\n",
    "                    price = f\"{val} {cur}\".strip()\n",
    "                else:\n",
    "                    price = None\n",
    "\n",
    "                # Category\n",
    "                category_el = item.find(\"span\", class_=\"ps-tag\")\n",
    "                category = category_el.get_text(strip=True) if category_el else None\n",
    "\n",
    "                # Date\n",
    "                date_el = item.find(\"span\", class_=\"cardinfo__datetime\")\n",
    "                date = date_el.get_text(strip=True) if date_el else None\n",
    "\n",
    "                # Location\n",
    "                loc_el = item.find(\"span\", class_=\"cardinfo__location\")\n",
    "                location = loc_el.get_text(strip=True) if loc_el else None\n",
    "                city = location.split(\",\")[-1].strip() if location else None\n",
    "\n",
    "                data.append([title, category, date, price, city])\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # ‚úÖ Save results into CSV\n",
    "    df = pd.DataFrame(data, columns=[\"title\", \"category\", \"date\", \"price\", \"location\"])\n",
    "    df.to_csv(\"fastli_clean.csv\", index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ Scraping completed! {len(df)} listings found. Saved to fastli_clean.csv.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Usage\n",
    "df = await scrape_listings(\"https://fastli.pl/missions?page=1\", max_pages=20)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170f903-f7d8-4b5b-960f-143aede85df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
